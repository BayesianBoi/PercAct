---
title: "221212-PercActExam"
author: "Thomas Steinthal"
date: "2022-12-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


pacman::p_load(tidyverse, mousetrap, readbulk, stringr)
raw_df <- read_opensesame(directory = "data/", extension = ".csv")
```

FUNCTIONS

```{r}
#Fill out the valence and arousal columns
time_thirteen_fun <- function(raw_df) {
sorter <- raw_df %>% na.omit() %>% select(arousal, valence)
sorter <- tibble(arousal = rep(sorter$arousal, each=13), 
                 valence = rep(sorter$valence, each=13))
return(sorter)
}







# Cleaning of id variable and input of code-name
nam_fun <- function(file) {
nam <- read.delim(file,sep=",") 
name<- as.list(strsplit(toString(colnames(nam)), ",")[[1]])
names<- tibble(ID = rep(name, each=12))

return (names)
}
```

CODING

```{r}
sorter <- time_thirteen_fun(raw_df)

names <- nam_fun("id-codes.txt")

### BUILD THE DATASET ### 
df <- tibble(
  id = as.numeric(gsub(".*?([0-9]+).*", "\\1", raw_df$File)), #Unique ID as num
  trial = raw_df$count_logger, #Trial
  gender = as.factor(raw_df$gender), #Gender as factor
  age = raw_df$age, #age
  sorter[1], #arousal
  sorter[2], #valence
  init_time = raw_df$initiation_time_mouse_tracker, #Initiation time of the trial
  resp_time = raw_df$response_time_mouse_tracker, #Response time
  level = raw_df$level_of_stim, #Level of stimuli
  list_time = raw_df$timestamps_mouse_tracker, #Timestamp for the mouse-tracking data
  xp = raw_df$xpos_mouse_tracker, #X-positions
  yp = raw_df$ypos_mouse_tracker, #Y-positions
  date = raw_df$datetime #When
) %>% 
  filter(row_number() %% 13 != 0) %>%  ## Delete every 13rd row starting from 0 to get rid of the replicate row) 
  arrange(id) %>% 
  mutate(ID = names$ID)

```


```{r}
### WE'LL ALSO CREATE A DATASET FOR THE MOUSETRAP-DATA
trap_df <- raw_df %>% 
  mutate(arousal = time_thirteen_fun(raw_df)[1],
         valence = time_thirteen_fun(raw_df)[2],
         id = as.numeric(gsub(".*?([0-9]+).*", "\\1", raw_df$File))) %>% 
  filter(row_number() %% 13 != 0) %>% 
  arrange(id) %>% 
  mutate(ID = names$ID)
  
## And from here we create the mouse-trap object
trap_obj <- mt_import_mousetrap(trap_df)

```



This was the initial cleaning. We now move on to the minor analysis

### MINOR ANALYSIS ###

```{r}
#Initial analysis - Mean emotion for gender
df %>% group_by(gender) %>% summarise(mean(arousal), mean(valence))
df %>% group_by(gender) %>% summarise(sd(arousal), sd(valence))

df %>% count(gender) %>% mutate(n = n/12)

#Initial analysis - Age 
df %>% count(age) %>% mutate(n = n/12)
sd(df$age)
mean(df$age)

```

### MACHINE LEARNING PIPELINE ###
## Data extraction
Each trial needs to be extracted into the measures from the Yamanuchi et al. (2016) paper. 

FUNCTIONS FOR THE DATA EXTRACTION AND CLEANING
```{r}
trial_setting <- function(data, dd, tri) {
df_T2 <- subset(data,id == dd & trial == tri)
#Extracting the time, x and y and making them columns in a new data frame
extract_num = function(x){as.numeric(gsub("[^0-9.\\-]+","",as.character(x)))} #Function for extracting numbers

t_df <- tibble( #First we extract the elements we want to parse
  time = as.list(strsplit(as.character(df_T2[1,"list_time"]), ",")[[1]]),
  xpos = as.list(strsplit(as.character(df_T2[1,"xp"]), ",")[[1]]),
  ypos = as.list(strsplit(as.character(df_T2[1,"yp"]), ",")[[1]]),
) 
 t_1_df <- t_df %>% 
   mutate( #Cleaning for shit
          time = extract_num(t_df$time),
          xpos = extract_num(t_df$xpos),
          ypos = extract_num(t_df$ypos)
 ) %>% 
   filter(!xpos %in% (-5:5) #Filtering off the zeros
 ) %>% 
   mutate(time = time - time[1]) #And readjusting the time
 
t_1_df <- t_1_df %>% #Putting on the ID and trial variable
   mutate(
     ID = as.character(df_T2$ID[1]),
     trial = df_T2$trial[1]  
 )
return(t_1_df)
}








## This function 'pseudo-splits' the dataset in splits as inputted
make_part_fun <- function(df, splits){

gue <- floor(length(df$time)/splits) #First we find the closest to an equal split rounded down
#Then we awkwardly do a while-loop to find out how close we guessed
miss <- 0
while(length(df$time) != (gue*splits)+miss){
  miss <- miss+1
}
#And finally we create a list for the new indexing
part <- c(rep(1:splits, each=gue),rep(splits, each=miss))

#...before binding
df <- cbind(df, part)
return(df)
}







dir_dev_calc_fun <- function(trial_df, n_splits){
df<- tibble( #The empty dataframe for input
)

for(i in 1:n_splits) { #For each part...
t_df <- trial_df %>% filter(part == i) 
m<-lm(ypos ~ xpos, t_df) #... make a linear model
DM<-as.numeric(m$coefficients[2]) #direction mean 
DSD<-sqrt(diag(vcov(m)))[2] #direction_sd (predictor uncertainty)
MSR<-summary(m)$r.squared #deviation (MSR - mean sum of residuals)
RSE<-summary(m)$sigma #Residuals standard error

li<-tibble( #The tibble with all the required stuff!
  paste(as.character(trial_df$ID[1]),"-",trial_df$trial[1],"-",i),
  DM,
  DSD,
  MSR,
  RSE,
  part = i
  )
df<-rbind(df,li) #binding
}

df<-df %>% #Finally also putting on the ID and trial as seperates
  mutate(
    ID=as.character(trial_df$ID[1]),
    trial=trial_df$trial[1]
  )


return(df)
}













#Remove fault cases
rem_fau_fun <- function(df) {
  my_list<-c(1:length(read.delim("id-codes.txt",sep=","))) #How many participants do we have?
  data_fr <- df %>% filter(resp_time == 0) %>% select(id) #If participants is fucked filter them out
  rem_list = pull(data_fr,1) #Convert to vector
  rem_list[!duplicated(rem_list)] #Remove duplicates
  
  my_list<-my_list[!my_list %in% rem_list] #Remove from the original list! 
  
  return(my_list)
}


#The same function that returns the opposite
rem_fau_fun_2 <- function(df) {
  my_list<-c(1:length(read.delim("id-codes.txt",sep=","))) #How many participants do we have?
  data_fr <- df %>% filter(resp_time == 0) %>% select(id) #If participants is fucked filter them out
  rem_list = pull(data_fr,1) #Convert to vector
  rem_list[!duplicated(rem_list)] #Remove duplicates
  return(rem_list)
}











#As big_data_fun, but for the participants that failed
big_data_supply_fun <- function(n_splits,df){
df_dum <- df %>% filter(id != 44)
return_df <- tibble() #For the final return

use_list <- rem_fau_fun_2(df_dum) #All the fucked participants 
use_list<-use_list[!duplicated(use_list)] #Remove duplicates

fau_trial_df <- df_dum %>% filter(resp_time == 0) %>% select(id,trial) #If participants is fucked find the trial

for (i in 1:length(use_list)) {
ret_df <- tibble()
id <- use_list[i]

  for (trial_i in 1:12){  
    if(trial_i != fau_trial_df[i,2]) { #Using the fau_trail_df, we can sort out the failed cases
  df_1 <- trial_setting(df, id, trial_i) #Pick your trial and get the coordinates

  df_2 <- make_part_fun(df_1,n_splits) #Split the data-set in equal parts

  df_3<-dir_dev_calc_fun(df_2, n_splits) #Calculate the direction of the n parts

  ret_df<-rbind(ret_df,df_3)
    } else {}
  }
return_df<-rbind(return_df,ret_df) 

}

return(return_df)
}
```

ACTUAL CODING FOR THE DATA EXTRACTION AND CLEANING
```{r, warning = F, message = F}
big_data_fun <- function(n_splits,df){
return_df <- tibble() #For the final return

use_list <- rem_fau_fun(df)

for (i in 1:length(use_list)) {
ret_df <- tibble()
id <- use_list[i]
  for (trial_i in 1:12){  
  df_1 <- trial_setting(df, id, trial_i) #Pick your trial and get the coordinates

  df_2 <- make_part_fun(df_1,n_splits) #Split the data-set in equal parts

  df_3<-dir_dev_calc_fun(df_2, n_splits) #Calculate the direction of the n parts

  ret_df<-rbind(ret_df,df_3)
  }
return_df<-rbind(return_df,ret_df) 
}

return(return_df)
}

big_data_1<-big_data_fun(4,df) #Create the main part of the dataset
big_data_sup <-big_data_supply_fun(3,df) #Create the supply (the participant with missing participants)
 
big_data_2 <- rbind(big_data_1,big_data_sup) #Binding

big_data_2 <- big_data_2 %>%  #And renaming to make stuff look nice!
  rename("GID" = `paste(...)`)

# removing NA's from the data set
big_data_3 <- replace(big_data_2, is.na(big_data_2), 0)

# cleaning up the clutter
rm(big_data_1, big_data_sup, big_data_2, names, raw_df, sorter, trap_df, trap_obj)
```

Behavioral Analysis (Did people go for holistic vs. whollistic features?)
```{r}
behav1 <- raw_df %>% 
  select(
    gender,
    response_time_mouse_tracker,
    level_of_stim,
    response,
    left_stimuli,
    right_stimuli,
    ref_stimuli,
    File,
    valence
  )


# Convert 'left_stimuli', 'ref_stimuli', and 'right_stimuli' to numeric
convert_to_numeric <- function(df, col) {
  df[[col]] <- as.numeric(gsub("[^[:digit:], ]", "", df[[col]]))
}


for (col in c("left_stimuli", "ref_stimuli", "right_stimuli")) {
  convert_to_numeric(behav1, col)
}

# Create 'right_answer' and 'left_answer' columns
behav1$right_answer <- ifelse(
  behav1$right_stimuli == behav1$ref_stimuli, "Holistic", "Wholistic"
)

behav1$left_answer<- ifelse(
  behav1$right_stimuli == behav1$ref_stimuli, "Wholistic", "Holistic"
  )


# Remove rows with response == "space"
behav1 <- behav1[behav1$response != "space", ]

# Creating a column sorting people into either happy (valence > 5), neural (v = 5), or sad (v < 5)


# Create 'answer' column
behav1$answer <- ifelse(
  behav1$response == "left", behav1$left_answer, behav1$right_answer
)

# Convert 'level_of_stim' to factor
behav1$level_of_stim <- as.factor(behav1$level_of_stim)

# Filter rows with response time over 4000ms
behav1 <-  behav1[which(behav1$response_time_mouse_tracker <= 4000),]

# Split data by gender
behav1_male <- behav1[behav1$gender == "Male", ]
behav1_female <- behav1[behav1$gender == "Female", ]



# Calculate proportions and standard deviations for male and female data

prop_male_wholistic <- behav1_male %>%
  group_by(level_of_stim) %>%
  summarize(prop_global = mean(answer == "Wholistic"),
            prop_sd = sqrt(prop_global * (1 - prop_global)/length(answer)))

prop_male_holistic <- behav1_male %>%
  group_by(level_of_stim) %>%
  summarize(prop_global = mean(answer == "Holistic"),
            prop_sd = sqrt(prop_global * (1 - prop_global)/length(answer)))

prop_female_wholistic <- behav1_female %>%
  group_by(level_of_stim) %>%
  summarize(prop_global = mean(answer == "Wholistic"),
            prop_sd = sqrt(prop_global * (1 - prop_global)/length(answer)))

prop_female_holistic <- behav1_female %>%
  group_by(level_of_stim) %>%
  summarize(prop_global = mean(answer == "Holistic"),
            prop_sd = sqrt(prop_global * (1 - prop_global)/length(answer)))

# Calculating time mean and SD for both genders
time_male <- behav1_male %>%
  group_by(level_of_stim) %>%
  summarize(mean_time = mean(response_time_mouse_tracker),
            time_sd = sd(response_time_mouse_tracker))

time_female <- behav1_female %>%
  group_by(level_of_stim) %>%
  summarize(mean_time = mean(response_time_mouse_tracker),
            time_sd = sd(response_time_mouse_tracker))
  

# PLOTTING
# Female holistic answers  
# Function to create a plot of proportions and standard deviations
plot_props <- function(df, title, y_axis_label) {
  ggplot(data = df, aes(x = level_of_stim, y = prop_global, fill = level_of_stim)) +
    geom_bar(stat = "identity") +
    geom_errorbar(aes(ymin = prop_global - prop_sd, ymax = prop_global + prop_sd)) +
    labs(x = "Level of Stimulus", y = y_axis_label) +
    theme_minimal() +
    scale_x_discrete(limits = c("1", "2", "3")) +
    scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +
    scale_fill_brewer(name = "Stimulus Level", palette = 1) +
    labs(title = title)
}

# Function to create a plot of mean response times and standard deviations
plot_time <- function(df, title) {
  ggplot(data = df, aes(x = level_of_stim, y = mean_time, fill = level_of_stim)) +
    geom_bar(stat = "identity") +
    geom_errorbar(aes(ymin =  mean_time - time_sd, ymax =  mean_time + time_sd)) +
    labs(x = "Level of Stimulus", y = "Response time (ms)") +
    theme_minimal() +
    scale_x_discrete(limits = c("1", "2", "3")) +
    scale_fill_brewer(name = "Stimulus Level", palette = 5) +
    labs(title = title)
}

# Plot female holistic answers
plot_female_hol <- plot_props(prop_female_holistic, "Female (a)", "Proportion of Holistic answers")

# Plot male holistic answers
plot_male_hol <- plot_props(prop_male_holistic, "Male (b)", "Proportion of Holistic answers")

# Plot female response times
plot_femaletime <- plot_time(time_female, "Female (c)")

# Plot male response times
plot_maletime <- plot_time(time_male, "Male (d)")


t.test()
# Arrange plots in a grid
grid.arrange(plot_female_hol, plot_male_hol, plot_femaletime, plot_maletime)
```

```{r}
# Create a data frame containing all the demographics of the participants
df_demo <- df %>% 
  select(valence, arousal, age, gender, ID)

# Merge the big data frame with the demographics
big_data_full <- big_data_3 %>% 
  merge(df_demo, by = "ID") %>% 
  # Remove duplicated rows
  unique() %>% 
  # Modify the data frame for machine learning
  # Select columns to keep
  select(-c(age, trial, part, GID)) %>% 
  # Convert ID to factor and arousal and valence to numeric
  mutate(ID = as.factor(ID),
         arousal = as.numeric(arousal),
         valence = as.numeric(valence))
```

```{r}
set.seed(1)
pacman::p_load(tidymodels, parsnip, vip, caret, car, tune, workflows, conflicted, forcats, patchwork, caretEnsemble, caTools, psych, ranger, broom)

# Split the data into training and test sets based on ID, so that the same ID only is present in one of the data sets (to avoid overfitting)
# 80/20 split
set.seed(14)
big_data_split <- group_initial_split(big_data_full, ID, prop = 4/5)
train_data <- training(big_data_split)
test_data <- testing(big_data_split)

# Create a recipe to predict valence
rec_model <- train_data %>% 
  recipe(valence ~ .) %>% 
  update_role(ID, new_role = "ID") %>%
  update_role(valence, new_role = "outcome") %>% 
  update_role(arousal, new_role = "secondary") %>% 
  update_role(DM, DSD, MSR, RSE, new_role = "predictors") %>% 
  step_center(MSR, DSD, DM, RSE) %>%
  step_scale(MSR, DSD, DM, RSE) %>% 
  # Prepare the recipe
  prep(retain = T)

# Juice the train data set
train_data_s <- juice(rec_model)

# Bake the test data set
test_data_s <- bake(rec_model, new_data = test_data)

# Clean up
rm(big_data_split, train_data, test_data)

# RF model - specify variable importance for valence
rf_res_valence <- ranger(valence ~ MSR+ DSD+ DM+ RSE, data = train_data_s, importance = "impurity_corrected")

# RF model - specify variable importance for arousal
rf_res_arousal <- ranger(arousal ~ MSR+ DSD+ DM+ RSE, data = train_data_s, importance = "impurity_corrected")

# Plot variable importance for valence
# imp_valence <- importance(rf_res_valence) %>% 
#     enframe("Variable", "Importance") %>%
#     mutate(Variable = fct_reorder(Variable, Importance),
#            New = Variable %in% setdiff(names(train_data_s), names(big_data_full))) %>% 
#     arrange(desc(Importance)) %>% 
#     ggplot(aes(x = Variable, y = Importance, fill = New)) +
#     geom_col(fill = "#87C1FF") +
#     coord_flip() +
#     theme_minimal() +
#     scale_fill_viridis_d(end = .4) +
#     labs(title = "Variable Importance - Valence", subtitle = "Original Variables")

# plotting variable importance for arousal
imp_arousal <- ranger::importance(rf_res_arousal) %>% 
    enframe("Variable", "Importance") %>%
    mutate(Variable = fct_reorder(Variable, Importance),
           New = Variable  %in% setdiff(names(train_data_s), names(big_data_full))) %>% 
    arrange(desc(Importance)) %>% 
    ggplot(aes(x = Variable, y = Importance, fill = New)) +
    geom_col(fill = "#90EE90") +
    coord_flip() +
  theme_minimal()+
    scale_fill_viridis_d(end = .4) +
    labs(title = "Variable Importance - Arousal", subtitle = "Original Variables")

grid.arrange(imp_arousal, imp_valence)
# removing clutter
rm(rf_res_arousal, rf_res_valence)

# defining correlation plot 
cr <- train_data_s %>% 
    select(DSD, MSR, DM, RSE, arousal, valence) %>% 
    cor(use = "pair")

# modifying correlation plot for valence
cr1 <- cr[order(cr[, "valence"], decreasing = T),
           order(cr[, "valence"], decreasing = T)]

corrplot::corrplot(cr1[1:6, 1:6], type = "full", method = "square")


# removing clutter
rm(cr1, cr)

# exploratory random forest regression
rf_mod <- 
  rand_forest(trees = 500, mode = "regression") %>% 
  set_engine("ranger")

#fitting the model on the valence training data
rf_fit_valence <- 
  rf_mod %>% 
  fit(valence ~ DM+ DSD+ RSE+ MSR, data = train_data_s)

# fitting the model on the arousal training data
rf_fit_arousal <- 
  rf_mod %>% 
  fit(arousal ~ DM+ DSD+ RSE+ MSR, data = train_data_s)


## EVALUATION
# getting the predictions for the valence for each data entry (multiple per participant)
rf_metrics_valence <- rf_fit_valence %>% 
  predict(test_data_s) %>% 
  bind_cols(test_data_s)

# getting the predictions for the arousal for each data entry (multiple per participant)
rf_metrics_arousal <- rf_fit_arousal %>% 
  predict(test_data_s) %>% 
  bind_cols(test_data_s)
  
# grouping the prediction metrics for each ID, and calculating the mean prediction
# valence
grouped_rf_mod1 <- rf_metrics_valence %>% 
  group_by(ID) %>% 
  summarise_at(vars(.pred), list(valence_pred = mean))

#arousal
grouped_rf_mod2 <- rf_metrics_arousal %>% 
  group_by(ID) %>% 
  summarise_at(vars(.pred), list(arousal_pred = mean))

# getting the actual arousal and valence for each participant
actual_metrics <- test_data_s[!duplicated(test_data_s$ID),] %>% 
  select(ID, arousal, valence) %>% 
  rename("actual_arousal" = arousal) %>% 
  rename("actual_valence" = valence)

# merging the the predictions with the actual observations
final_metrics <- merge(grouped_rf_mod2, grouped_rf_mod1, by = "ID") %>% 
  merge(actual_metrics, by = "ID")

# Calculate RMSE and R-squared for valence
valence_metrics <- final_metrics %>% 
  metrics(truth = actual_valence, estimate = valence_pred)

# Calculate RMSE and R-squared for arousal
arousal_metrics <- final_metrics %>% 
  metrics(truth = actual_arousal, estimate = arousal_pred)

# Combine the RMSE and R Squared values for valence and arousal
combined_metrics <- full_join(valence_metrics, arousal_metrics)

```





## MACHINE LEARNING WITH OTHER MODELS
TESTING DIFFERENT MODELS
```{r}
# Splitting the data into test and train data sets based on the gender
train_data_s_m <- train_data_s[train_data_s$gender == "Male",]
test_data_s_m <- test_data_s[test_data_s$gender == "Male",]
train_data_s_f <- train_data_s[train_data_s$gender == "Female",]
test_data_s_f <- test_data_s[test_data_s$gender == "Female",]

# setting up the controls for the models, including cross validation, which takes one ID at a time and computes it against all the other IDs. (Change it manually based on whether we're testing the male or female group)
trControl <- trainControl(
  method = "cv", # for cross-validation
  savePredictions = "final",
  allowParallel = T,
  index =groupKFold(train_data_s_f$ID, k = length(unique(train_data_s_f$ID))),
   #number of kfolds and repeats
  verboseIter = F,
  seeds = set.seed(1))

# setting up the model list. Change it manually to set either arousal or valence as outcome, and whether it is the female or male group we're testing. Remember to change it in the code below also:
modelList <- caretList(
  arousal ~ DM+ DSD+ MSR+ RSE, data = train_data_s_f,
  verbose = F,
  metric = "RMSE",
  tuneList = NULL,
  trControl = trControl,
  methodList = c("svmRadial", "ranger", "xgbTree"))


# Extract the resampling results for each model
resampling_results <- resamples(modelList)

# Initialize a vector to store the average RMSE for each model
average_rmse <- numeric(length(modelList))

# Get the names of all of the models in the modelList object
model_names <- names(modelList)

# Initialize a list to store the predictions for each model
prediction_list <- list()
sd_list <- list()

# Loop over each model
for (i in seq_along(model_names)) {
  
  # Extract the model name and store it in a variable
  model_name <- model_names[i]
  
  # Extract the predictions for the current model using the predict function
  predictions <- predict(modelList[[model_name]], newdata = test_data_s_f)
  
  # Store the predictions in the prediction list
  prediction_list[[model_name]] <- predictions
}

# Initialize vectors to store the p-values and rank correlations for each model
model_names <- names(prediction_list)
p_values <- numeric(length(model_names))
rank_correlations <- numeric(length(model_names))

# Loop over each model
for (i in seq_along(model_names)) {
  
  # Extract the model name and store it in a variable
  model_name <- model_names[i]
  
  # Extract the predictions and observed values for the current model
  predictions <- prediction_list[[model_name]]
  observed_values <- test_data_s_f$arousal
  
  # Calculate the rank correlation between the predicted and observed values
  correlation_test_result <- cor.test(predictions, observed_values, method = "spearman")
  
  # Extract the rank correlation and p-value from the test result
  rank_correlation <- correlation_test_result$estimate
  p_value <- correlation_test_result$p.value
  

  # Store the p-value and rank correlation in the appropriate vectors
  p_values[i] <- p_value
  rank_correlations[i] <- rank_correlation
  
    # Getting SD for the rank correlations
    sd <- sd(rank_correlation)
  
  # Store the standard deviation in the sd_list
  sd_list[[model_name]] <- sd
}

# Assign names to the p-values and rank correlations vectors
names(p_values) <- model_names
names(rank_correlations) <- model_names
names(sd_list) <- model_names

# Getting the rank correlations and p-values
print(p_values)
print(rank_correlations)
print(sd_list)



# getting RMSE and rsq for the models
modelResults <- data.frame(
  SVM_RMSE = min(modelList$svmRadial$results$RMSE),
  SVM_rsq = max(modelList$svmRadial$results$Rsquared),
  RF_RMSE = min(modelList$ranger$results$RMSE),
  RF_rsq = max(modelList$ranger$results$Rsquared),
  XGBT_RMSE = min(modelList$xgbTree$results$RMSE),
  XGBT_rsq =max(modelList$xgbTree$results$Rsquared)
)
print(modelResults)

```






